==== File: /home/stefan/repos/abenteuer-auf-der-dino-insel/scripts/repo_snapshot.sh ====

#!/usr/bin/env bash
set -euo pipefail

# Usage: scripts/repo_snapshot.sh <REPO_ROOT> <OUTPUT_FILE>
if [[ $# -ne 2 ]]; then
  echo "Usage: $0 <REPO_ROOT> <OUTPUT_FILE>"
  exit 1
fi

REPO_ROOT=$1
OUTFILE=$2

SCRIPT_DIR="$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" && pwd)"
UTIL="${SCRIPT_DIR}/utils/concat_files.sh"

[[ -x "$UTIL" ]] || { echo "Utility not executable: $UTIL" >&2; exit 1; }
[[ -d "$REPO_ROOT" ]] || { echo "Repo root not found: $REPO_ROOT" >&2; exit 1; }

# Hier kannst du repo-spezifische Zusatz-Excludes angeben (optional):
# EXTRA_NAME_EXCLUDES=( "node_modules" ".mypy_cache" )
EXTRA_NAME_EXCLUDES=()
EXTRA_PATH_EXCLUDES=()

# Beispiel: nur bestimmte Dateitypen aufnehmen – kommentiere aus wenn gewünscht
# FIND_FILTER=( -name "*.adoc" -o -name "*.md" )
FIND_FILTER=()

# Aufruf Utility: .git und .venv immer raus; plus evtl. Extras
# -- sichert, dass alles nach dem Trenner 1:1 bei find landet (z. B. -maxdepth etc.)
"$UTIL" "$REPO_ROOT" "$OUTFILE" \
  --exclude .git \
  --exclude .venv \
  $(for n in "${EXTRA_NAME_EXCLUDES[@]}"; do printf -- "--exclude %q " "$n"; done) \
  $(for p in "${EXTRA_PATH_EXCLUDES[@]}"; do printf -- "--exclude-path %q " "$p"; done) \
  -- \
  "${FIND_FILTER[@]}"


==== File: /home/stefan/repos/abenteuer-auf-der-dino-insel/scripts/utils/concat_files.sh ====

#!/usr/bin/env bash
set -euo pipefail

echo "Generating file snapshot"

# ---------------------------------
# Defaults
# ---------------------------------
DEFAULT_NAME_EXCLUDES=( ".git" ".venv" )
USE_DEFAULT_EXCLUDES=1

# ---------------------------------
# Usage
# ---------------------------------
if [[ $# -lt 2 ]]; then
  echo "Usage: $0 <DIRECTORY> <OUTPUT_FILE> [options & find-args...]"
  echo
  echo "Options (mehrfach nutzbar, Reihenfolge egal):"
  echo "  --exclude NAME            # Verzeichnisname ignorieren (z. B. node_modules)"
  echo "  --exclude-path GLOB       # Pfad-Glob, z. B. '*/build/*' (für -path)"
  echo "  --exclude-file FILE       # Datei, 1 Muster/Z.: NAME:<dir> | PATH:<glob> | autodetect"
  echo "  --no-default-excludes     # .git und .venv NICHT automatisch ausschließen"
  echo "  --                        # Trenner; alles danach geht 1:1 an find"
  exit 1
fi

DIRECTORY=$1
OUTPUT_FILE=$2
shift 2

# --- kleine Helfer für absolute Pfade ---
_abspath() {
  # realpath, falls vorhanden
  if command -v realpath >/dev/null 2>&1; then
    realpath -m -- "$1"
  else
    # Fallback: cd + pwd
    ( cd -- "$(dirname -- "$1")" && printf '%s/%s\n' "$(pwd)" "$(basename -- "$1")" )
  fi
}

DIRECTORY_ABS=$(_abspath "$DIRECTORY")
OUTPUT_FILE_ABS=$(_abspath "$OUTPUT_FILE")
OUTPUT_DIR_ABS=$(_abspath "$(dirname -- "$OUTPUT_FILE")")

NAME_EXCLUDES=()
PATH_EXCLUDES=()
FIND_OPTIONS=()

read_exclude_file() { # ... (unverändert wie bei dir) ...
  local f="$1"
  [[ -f "$f" ]] || { echo "Exclude file not found: $f" >&2; exit 1; }
  while IFS= read -r line || [[ -n "$line" ]]; do
    [[ -z "${line// }" ]] && continue
    [[ "$line" =~ ^[[:space:]]*# ]] && continue
    if [[ "$line" == PATH:* ]]; then
      PATH_EXCLUDES+=( "${line#PATH:}" )
    elif [[ "$line" == NAME:* ]]; then
      NAME_EXCLUDES+=( "${line#NAME:}" )
    else
      if [[ "$line" == */* ]]; then PATH_EXCLUDES+=( "$line" ); else NAME_EXCLUDES+=( "$line" ); fi
    fi
  done < "$f"
}

while (( $# )); do
  case "${1:-}" in
    --exclude)        shift; [[ $# -gt 0 ]] || { echo "--exclude braucht ein Argument" >&2; exit 1; }; NAME_EXCLUDES+=( "$1" ) ;;
    --exclude-path)   shift; [[ $# -gt 0 ]] || { echo "--exclude-path braucht ein Argument" >&2; exit 1; }; PATH_EXCLUDES+=( "$1" ) ;;
    --exclude-file)   shift; [[ $# -gt 0 ]] || { echo "--exclude-file braucht ein Argument" >&2; exit 1; }; read_exclude_file "$1" ;;
    --no-default-excludes) USE_DEFAULT_EXCLUDES=0 ;;
    --) shift; while (( $# )); do FIND_OPTIONS+=( "$1" ); shift; done; break ;;
    *)  FIND_OPTIONS+=( "$1" ) ;;
  esac
  shift || true
done

if [[ "$USE_DEFAULT_EXCLUDES" -eq 1 ]]; then
  NAME_EXCLUDES=( "${DEFAULT_NAME_EXCLUDES[@]}" "${NAME_EXCLUDES[@]}" )
fi

[[ -d "$DIRECTORY_ABS" ]] || { echo "Error: Directory '$DIRECTORY' does not exist." >&2; exit 1; }
: > "$OUTPUT_FILE_ABS"

# ----------------- PRUNES bauen -----------------
PRUNE_NAME_EXPR=()
if (( ${#NAME_EXCLUDES[@]} )); then
  PRUNE_NAME_EXPR+=( -type d '(' )
  for i in "${!NAME_EXCLUDES[@]}"; do
    PRUNE_NAME_EXPR+=( -name "${NAME_EXCLUDES[$i]}" )
    (( i < ${#NAME_EXCLUDES[@]} - 1 )) && PRUNE_NAME_EXPR+=( -o )
  done
  PRUNE_NAME_EXPR+=( ')' -prune -false -o )
fi

PRUNE_PATH_EXPR=()
if (( ${#PATH_EXCLUDES[@]} )); then
  PRUNE_PATH_EXPR+=( '(' )
  for i in "${!PATH_EXCLUDES[@]}"; do
    PRUNE_PATH_EXPR+=( -path "${PATH_EXCLUDES[$i]}" )
    (( i < ${#PATH_EXCLUDES[@]} - 1 )) && PRUNE_PATH_EXPR+=( -o )
  done
  PRUNE_PATH_EXPR+=( ')' -prune -false -o )
fi

PRUNE_OUTFILE_EXPR=( -path "$OUTPUT_FILE_ABS" -prune -o )


# Optional: auch das Output-Verzeichnis prunen (meist nicht nötig, kann aber Build-Verzeichnisse sauber halten)
# PRUNE_OUTDIR_EXPR=( -path "$OUTPUT_DIR_ABS/*" -prune -false -o )

# ----------------- find & Verarbeitung -----------------
# Tipp: robust gegen Spaces mit -print0
find "$DIRECTORY_ABS" \
  "${FIND_OPTIONS[@]}" \
  "${PRUNE_NAME_EXPR[@]}" \
  "${PRUNE_PATH_EXPR[@]}" \
  "${PRUNE_OUTFILE_EXPR[@]}" \
  -type f -print0 \
| while IFS= read -r -d '' FILE; do
    if file --mime-encoding "$FILE" | grep -q 'utf-8'; then
      echo "Processing $FILE"
      {
        echo "==== File: $FILE ===="
        echo
        cat "$FILE"
        echo
      } >> "$OUTPUT_FILE_ABS"
    else
      echo "Skipping non-UTF-8 file: $FILE"
    fi
  done

echo "All files concatenated into $OUTPUT_FILE_ABS"


==== File: /home/stefan/repos/abenteuer-auf-der-dino-insel/scripts/utils/old/concat_files_old2.sh ====

#!/usr/bin/env bash
set -euo pipefail

# ---------------------------------
# Defaults
# ---------------------------------
DEFAULT_NAME_EXCLUDES=( ".git" ".venv" )
USE_DEFAULT_EXCLUDES=1

# ---------------------------------
# Usage
# ---------------------------------
if [[ $# -lt 2 ]]; then
  echo "Usage: $0 <DIRECTORY> <OUTPUT_FILE> [options & find-args...]"
  echo
  echo "Options (vor beliebigen find-Args möglich, mehrfach nutzbar):"
  echo "  --exclude NAME            # Verzeichnisname global ignorieren (z.B. node_modules)"
  echo "  --exclude-path GLOB       # Pfad-Glob, z.B. '*/build/*' (für -path)"
  echo "  --exclude-file FILE       # Datei mit Zeilen: NAME  oder  PATH:GLOB"
  echo "  --no-default-excludes     # .git und .venv NICHT automatisch ausschließen"
  echo "  --                        # Trenner; alles danach geht 1:1 an find"
  exit 1
fi

DIRECTORY=$1
OUTPUT_FILE=$2
shift 2

# ---------------------------------
# CLI-Parsing
# ---------------------------------
NAME_EXCLUDES=()
PATH_EXCLUDES=()
FIND_OPTIONS=()

read_exclude_file() {
  local f="$1"
  [[ -f "$f" ]] || { echo "Exclude file not found: $f" >&2; exit 1; }
  while IFS= read -r line || [[ -n "$line" ]]; do
    # Kommentare und Leerzeilen überspringen
    [[ -z "${line// }" ]] && continue
    [[ "$line" =~ ^[[:space:]]*# ]] && continue
    # Optionales Format: PATH:<glob>  oder  NAME:<dir>
    if [[ "$line" == PATH:* ]]; then
      PATH_EXCLUDES+=( "${line#PATH:}" )
    elif [[ "$line" == NAME:* ]]; then
      NAME_EXCLUDES+=( "${line#NAME:}" )
    else
      # Heuristik: alles ohne Slash als NAME, sonst als PATH-Glob
      if [[ "$line" == */* ]]; then
        PATH_EXCLUDES+=( "$line" )
      else
        NAME_EXCLUDES+=( "$line" )
      fi
    fi
  done < "$f"
}

while (( $# )); do
  case "${1:-}" in
    --exclude)
      shift
      [[ $# -gt 0 ]] || { echo "--exclude braucht ein Argument" >&2; exit 1; }
      NAME_EXCLUDES+=( "$1" )
      ;;
    --exclude-path)
      shift
      [[ $# -gt 0 ]] || { echo "--exclude-path braucht ein Argument" >&2; exit 1; }
      PATH_EXCLUDES+=( "$1" )
      ;;
    --exclude-file)
      shift
      [[ $# -gt 0 ]] || { echo "--exclude-file braucht ein Argument" >&2; exit 1; }
      read_exclude_file "$1"
      ;;
    --no-default-excludes)
      USE_DEFAULT_EXCLUDES=0
      ;;
    --)
      shift
      # Rest unverändert an find weiterreichen
      while (( $# )); do FIND_OPTIONS+=( "$1" ); shift; done
      break
      ;;
    *)
      # Unbekannt → direkt an find weitergeben (Reihenfolge bleibt erhalten)
      FIND_OPTIONS+=( "$1" )
      ;;
  esac
  shift || true
done

# Defaults anhängen (falls nicht deaktiviert)
if [[ "$USE_DEFAULT_EXCLUDES" -eq 1 ]]; then
  NAME_EXCLUDES=( "${DEFAULT_NAME_EXCLUDES[@]}" "${NAME_EXCLUDES[@]}" )
fi

# ---------------------------------
# Checks & Vorbereitung
# ---------------------------------
if [[ ! -d "$DIRECTORY" ]]; then
  echo "Error: Directory '$DIRECTORY' does not exist." >&2
  exit 1
fi
: > "$OUTPUT_FILE"

# ---------------------------------
# find-Ausdrücke bauen
# ---------------------------------
# 1) Name-basierte Verzeichnis-Excludes: -type d \( -name X -o -name Y \) -prune -false -o
PRUNE_NAME_EXPR=()
if (( ${#NAME_EXCLUDES[@]} )); then
  PRUNE_NAME_EXPR+=( -type d '(' )
  for i in "${!NAME_EXCLUDES[@]}"; do
    PRUNE_NAME_EXPR+=( -name "${NAME_EXCLUDES[$i]}" )
    (( i < ${#NAME_EXCLUDES[@]} - 1 )) && PRUNE_NAME_EXPR+=( -o )
  done
  PRUNE_NAME_EXPR+=( ')' -prune -false -o )
fi

# 2) Pfad-Globs (können auch Dateien treffen): \( -path '*/build/*' -o -path '*/dist/*' \) -prune -false -o
PRUNE_PATH_EXPR=()
if (( ${#PATH_EXCLUDES[@]} )); then
  PRUNE_PATH_EXPR+=( '(' )
  for i in "${!PATH_EXCLUDES[@]}"; do
    PRUNE_PATH_EXPR+=( -path "${PATH_EXCLUDES[$i]}" )
    (( i < ${#PATH_EXCLUDES[@]} - 1 )) && PRUNE_PATH_EXPR+=( -o )
  done
  PRUNE_PATH_EXPR+=( ')' -prune -false -o )
fi

# ---------------------------------
# find & Verarbeitung
# ---------------------------------
# Reihenfolge: Nutzer-Filter (FIND_OPTIONS) → feste PRUNEs → -type f
find "$DIRECTORY" \
  "${FIND_OPTIONS[@]}" \
  "${PRUNE_NAME_EXPR[@]}" \
  "${PRUNE_PATH_EXPR[@]}" \
  -type f \
| while IFS= read -r FILE; do
    if file --mime-encoding "$FILE" | grep -q 'utf-8'; then
      echo "Processing $FILE"
      {
        echo "==== File: $FILE ===="
        echo
        cat "$FILE"
        echo
      } >> "$OUTPUT_FILE"
    else
      echo "Skipping non-UTF-8 file: $FILE"
    fi
  done

echo "All files concatenated into $OUTPUT_FILE"


==== File: /home/stefan/repos/abenteuer-auf-der-dino-insel/scripts/utils/old/concat_files_old3.sh ====

#!/usr/bin/env bash
set -euo pipefail

# ---------------------------------
# Defaults
# ---------------------------------
DEFAULT_NAME_EXCLUDES=( ".git" ".venv" )
USE_DEFAULT_EXCLUDES=1

# ---------------------------------
# Usage
# ---------------------------------
if [[ $# -lt 2 ]]; then
  echo "Usage: $0 <DIRECTORY> <OUTPUT_FILE> [options & find-args...]"
  echo
  echo "Options (mehrfach nutzbar, Reihenfolge egal):"
  echo "  --exclude NAME            # Verzeichnisname ignorieren (z. B. node_modules)"
  echo "  --exclude-path GLOB       # Pfad-Glob, z. B. '*/build/*' (für -path)"
  echo "  --exclude-file FILE       # Datei, 1 Muster/Z.: NAME:<dir> | PATH:<glob> | autodetect"
  echo "  --no-default-excludes     # .git und .venv NICHT automatisch ausschließen"
  echo "  --                        # Trenner; alles danach geht 1:1 an find"
  exit 1
fi

DIRECTORY=$1
OUTPUT_FILE=$2
shift 2

NAME_EXCLUDES=()
PATH_EXCLUDES=()
FIND_OPTIONS=()

read_exclude_file() {
  local f="$1"
  [[ -f "$f" ]] || { echo "Exclude file not found: $f" >&2; exit 1; }
  while IFS= read -r line || [[ -n "$line" ]]; do
    [[ -z "${line// }" ]] && continue
    [[ "$line" =~ ^[[:space:]]*# ]] && continue
    if [[ "$line" == PATH:* ]]; then
      PATH_EXCLUDES+=( "${line#PATH:}" )
    elif [[ "$line" == NAME:* ]]; then
      NAME_EXCLUDES+=( "${line#NAME:}" )
    else
      if [[ "$line" == */* ]]; then PATH_EXCLUDES+=( "$line" ); else NAME_EXCLUDES+=( "$line" ); fi
    fi
  done < "$f"
}

while (( $# )); do
  case "${1:-}" in
    --exclude)        shift; [[ $# -gt 0 ]] || { echo "--exclude braucht ein Argument" >&2; exit 1; }; NAME_EXCLUDES+=( "$1" ) ;;
    --exclude-path)   shift; [[ $# -gt 0 ]] || { echo "--exclude-path braucht ein Argument" >&2; exit 1; }; PATH_EXCLUDES+=( "$1" ) ;;
    --exclude-file)   shift; [[ $# -gt 0 ]] || { echo "--exclude-file braucht ein Argument" >&2; exit 1; }; read_exclude_file "$1" ;;
    --no-default-excludes) USE_DEFAULT_EXCLUDES=0 ;;
    --) shift; while (( $# )); do FIND_OPTIONS+=( "$1" ); shift; done; break ;;
    *)  FIND_OPTIONS+=( "$1" ) ;;
  esac
  shift || true
done

if [[ "$USE_DEFAULT_EXCLUDES" -eq 1 ]]; then
  NAME_EXCLUDES=( "${DEFAULT_NAME_EXCLUDES[@]}" "${NAME_EXCLUDES[@]}" )
fi

[[ -d "$DIRECTORY" ]] || { echo "Error: Directory '$DIRECTORY' does not exist." >&2; exit 1; }
: > "$OUTPUT_FILE"

PRUNE_NAME_EXPR=()
if (( ${#NAME_EXCLUDES[@]} )); then
  PRUNE_NAME_EXPR+=( -type d '(' )
  for i in "${!NAME_EXCLUDES[@]}"; do
    PRUNE_NAME_EXPR+=( -name "${NAME_EXCLUDES[$i]}" )
    (( i < ${#NAME_EXCLUDES[@]} - 1 )) && PRUNE_NAME_EXPR+=( -o )
  done
  PRUNE_NAME_EXPR+=( ')' -prune -false -o )
fi

PRUNE_PATH_EXPR=()
if (( ${#PATH_EXCLUDES[@]} )); then
  PRUNE_PATH_EXPR+=( '(' )
  for i in "${!PATH_EXCLUDES[@]}"; do
    PRUNE_PATH_EXPR+=( -path "${PATH_EXCLUDES[$i]}" )
    (( i < ${#PATH_EXCLUDES[@]} - 1 )) && PRUNE_PATH_EXPR+=( -o )
  done
  PRUNE_PATH_EXPR+=( ')' -prune -false -o )
fi

find "$DIRECTORY" \
  "${FIND_OPTIONS[@]}" \
  "${PRUNE_NAME_EXPR[@]}" \
  "${PRUNE_PATH_EXPR[@]}" \
  -type f \
| while IFS= read -r FILE; do
    if file --mime-encoding "$FILE" | grep -q 'utf-8'; then
      echo "Processing $FILE"
      {
        echo "==== File: $FILE ===="
        echo
        cat "$FILE"
        echo
      } >> "$OUTPUT_FILE"
    else
      echo "Skipping non-UTF-8 file: $FILE"
    fi
  done

echo "All files concatenated into $OUTPUT_FILE"


==== File: /home/stefan/repos/abenteuer-auf-der-dino-insel/scripts/spreadsheet_handling/pyproject.toml ====

[project]
name = "spreadsheet-handling"
version = "0.1.0"
description = "JSON ↔ spreadsheet converter with multi-level headers"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
  "pandas",
  "XlsxWriter",
  "openpyxl",
]

[project.scripts]
json2sheet = "spreadsheet_handling.cli.json2sheet:main"
sheet2json = "spreadsheet_handling.cli.sheet2json:main"

[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools.package-dir]
"" = "src"

[tool.setuptools.packages.find]
where = ["src"]
include = ["spreadsheet_handling*"]


==== File: /home/stefan/repos/abenteuer-auf-der-dino-insel/scripts/spreadsheet_handling/README.md ====

# spreadsheet-handling

JSON ↔ spreadsheet converter with multi-level headers.  
This package lets you roundtrip nested JSON structures into tabular form (Excel/CSV/ODS) for human editing, and back into JSON for storage or version control.

---

## Features

- Convert JSON with nested objects into spreadsheets with **multi-level headers**.
- Support for multiple backends:
  - Excel (`xlsxwriter` / `openpyxl`)
  - CSV (planned)
  - ODS (planned, via `odfpy`)
- Roundtrip support (spreadsheet → JSON → spreadsheet).
- Helper columns for **human-friendly editing** (e.g. showing IDs alongside names, prefixed with `_` and stripped on export).
- Modular design:
  - `core/` → flattening/unflattening, refs, dataframe builders
  - `io_backends/` → output writers (Excel etc.)
  - `cli/` → command line entrypoints
- Can be embedded in larger projects or used standalone.

---

## Usage (CLI)

Install dependencies (in editable mode):

```bash
make venv
make deps-dev
```

Run a roundtrip conversion:

```bash
# JSON → spreadsheet
json2sheet scripts/spreadsheet_handling/examples/roundtrip_start.json \
    -o scripts/spreadsheet_handling/tmp/tmp.xlsx \
    --levels 3
```

```bash
# spreadsheet → JSON
sheet2json scripts/spreadsheet_handling/tmp/tmp.xlsx \
    -o scripts/spreadsheet_handling/tmp/tmp.json \
    --levels 3
```


The CLI commands json2sheet and sheet2json are installed into your venv as console scripts.

## Usage (Makefile)

The root project ships a Makefile with common tasks:

```bash
make run     # example roundtrip using examples/roundtrip_start.json
make test    # run pytest suite
make clean   # remove tmp, __pycache__, .pytest_cache, lockfiles
```

### Development

Requirements are split:

```
requirements.txt → runtime deps (pandas, xlsxwriter, openpyxl)

requirements-dev.txt → runtime + pytest, hypothesis, etc.
```

To update the lock files:

```
make freeze
make freeze-dev
```

Tests live under tests/ with data fixtures in tests/data/.

## Separation Into Its Own Repo

Right now this code lives under `/scripts/spreadsheet_handling` in a monorepo.

To separate it in the future:

Move the `scripts/spreadsheet_handling/` directory to its own repo root.

Keep the `src/` layout (already in place).

Keep `pyproject.toml`, `requirements*.txt`, and this `README.md`.

Adjust the `Makefile` (drop monorepo paths).

Optionally publish to:

PyPI (for pip installs): pip install .

Docker registry (if you want isolated runtime environments).

Consumers can then depend on it like:

```
[tool.poetry.dependencies]
spreadsheet-handling = { git = "https://github.com/you/spreadsheet-handling.git", tag = "v0.1.0" }
```

This way the package is self-contained and can grow independently.

==== File: /home/stefan/repos/abenteuer-auf-der-dino-insel/scripts/spreadsheet_handling/src/spreadsheet_handling.egg-info/PKG-INFO ====

Metadata-Version: 2.4
Name: spreadsheet-handling
Version: 0.1.0
Summary: JSON ↔ spreadsheet converter with multi-level headers
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: pandas
Requires-Dist: XlsxWriter
Requires-Dist: openpyxl

==== File: /home/stefan/repos/abenteuer-auf-der-dino-insel/scripts/spreadsheet_handling/src/spreadsheet_handling/cli/json2sheet.py ====

# cli/json2sheet.py
import json, argparse
from spreadsheet_handling.core.flatten import flatten_json
from spreadsheet_handling.core.df_build import build_df_from_records
from spreadsheet_handling.core.refs import add_helper_columns
from spreadsheet_handling.io_backends.excel_xlsxwriter import ExcelBackend  # oder via config wählen

ap = argparse.ArgumentParser()
ap.add_argument("input_json")
ap.add_argument("-o","--output", required=True)
ap.add_argument("--levels", type=int, required=True)
ap.add_argument("--backend", choices=["excel","csv","ods"], default="excel")
ap.add_argument("--config")  # optional: YAML mit Sheet-Definitionen, Referenzen, etc.
args = ap.parse_args()

data = json.load(open(args.input_json))
rows = data if isinstance(data, list) else [data]
records = [flatten_json(r) for r in rows]

# optional: Hilfsspalten aus config
# ref_specs = load_from_config(args.config) ...
# records = add_helper_columns(records, ref_specs)

df = build_df_from_records(records, levels=args.levels)

backend = ExcelBackend()  # später switch nach args.backend
backend.write(df, args.output, sheet_name="Daten")


==== File: /home/stefan/repos/abenteuer-auf-der-dino-insel/scripts/spreadsheet_handling/src/spreadsheet_handling/io_backends/base.py ====

# io_backends/base.py
from typing import Optional
import pandas as pd

class SpreadsheetBackend:
    def write(self, df: pd.DataFrame, path: str, sheet_name: str = "Daten") -> None:
        raise NotImplementedError

    def read(self, path: str, header_levels: int, sheet_name: Optional[str] = None) -> pd.DataFrame:
        raise NotImplementedError

    # optional: multi-sheet API
    def write_multi(self, sheets: dict[str, pd.DataFrame], path: str) -> None:
        for name, df in sheets.items():
            self.write(df, path, sheet_name=name)

    def read_multi(self, path: str, header_levels: int) -> dict[str, pd.DataFrame]:
        # Default: nur „Daten“ lesen; echte Multi-Sheet-Implementierungen können überschreiben
        return {"Daten": self.read(path, header_levels, sheet_name="Daten")}


==== File: /home/stefan/repos/abenteuer-auf-der-dino-insel/scripts/spreadsheet_handling/src/spreadsheet_handling/core/unflatten.py ====

import pandas as pd

def is_empty_header(x: str | None) -> bool:
    if x is None: return True
    s = str(x).strip()
    return s == "" or s.lower() in ("nan","none") or s.startswith("Unnamed:")

def set_nested(d, segs, value):
    cur = d
    for i, s in enumerate(segs):
        last = i == len(segs)-1
        if last:
            cur[s] = value
        else:
            cur = cur.setdefault(s, {})

def row_to_obj(paths: list[str], values: list[str]) -> dict:
    obj = {}
    for p, v in zip(paths, values):
        if v is None: continue
        if isinstance(v, str) and v.strip() == "": continue
        segs = p.split(".")
        if segs and segs[0].startswith("_"):   # Hilfsspalten nicht zurückschreiben
            continue
        set_nested(obj, segs, v)
    return obj

def df_to_objects(df: pd.DataFrame) -> list[dict]:
    # Header-Pfade bauen, leere/Unnamed-Zellen skippen
    paths = []
    for col in df.columns:
        segs = [str(s) for s in col if not is_empty_header(s)]
        if not segs:  # komplett leerer Header -> Spalte ignorieren
            paths.append(None)
        else:
            paths.append(".".join(segs))
    # Spalten mit None droppen
    keep = [i for i, p in enumerate(paths) if p is not None]
    df2 = df.iloc[:, keep]
    paths2 = [paths[i] for i in keep]

    out = []
    for _, row in df2.iterrows():
        obj = row_to_obj(paths2, row.values.tolist())
        if obj:
            out.append(obj)
    return out


